setwd("Dropbox/Courses//MITx/Analytics/")
# VIDEO 2
# Read in data
boston = read.csv("boston.csv")
str(boston)
# Plot observations
plot(boston$LON, boston$LAT)
# Tracts alongside the Charles River
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)
# Plot MIT
points(boston$LON[boston$TRACT==3531],boston$LAT[boston$TRACT==3531],col="red", pch=20)
# Plot polution
summary(boston$NOX)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=20)
plot(boston$LON, boston$LAT)
summary(boston$MEDV)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
plot(boston$LAT, boston$MEDV)
plot(boston$LON, boston$MEDV)
latlonlm = lm(MEDV ~ LAT + LON, data=boston)
summary(latlonlm)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
latlonlm$fitted.values
points(boston$LON[latlonlm$fitted.values >= 21.2], boston$LAT[latlonlm$fitted.values >= 21.2], col="blue", pch="$")
rm(list=ls(0))
rm(list=ls())
gerber <- read.csv
gerber <- read.csv("gerber.csv")
gerber <- read.csv("gerber.csv")
gerber <- read.csv("gerber.csv")
gerber <- read.csv("gerber.csv")
gerber <- read.csv("gerber.csv")
str(gerber)
table(gerber$voting)
108696/length(gerber)
108696.0/length(gerber)
108696/344084
table(gerber$voting, gerber$civicduty)
table(gerber$voting, gerber$civicduty, avg)
table(gerber$voting, gerber$civicduty, mean)
table(gerber$voting, gerber$civicduty, mean())
table(gerber$voting &gerber$civicduty)
table(gerber$voting & gerber$civicduty)
table(gerber$voting & gerber$hawthorne)
table(gerber$voting & gerber$self)
table(gerber$voting & gerber$neighbors)
log.model <- lm(voting ~ civicduty+ hawthorne+ self + neighbors, family=binomial)
log.model <- lm(voting ~ civicduty+ hawthorne+ self + neighbors, family=binomial, data=gerber)
log.model <- glm(voting ~ civicduty+ hawthorne+ self + neighbors, family=binomial, data=gerber)
summary(log.model)
log.pred <- predict(log.model)
table(log.pred, log.pred>=0.3)
table(log.pred, log.pred>=0.3)
table(gerber$voting, log.pred>=0.3)
table(gerber$voting, log.pred>0.3)
str(log.pred)
table(gerber$voting, log.pred>0.3)
log.pred <- predict(log.model)
table(gerber$voting, log.pred>0.3)
table(gerber$voting)
log.pred <- predict(log.model)
log.pred <- predict(log.model)
table(gerber$voting)
table(gerber$voting, log.model)
table(gerber$voting, log.model > 0.3)
table(gerber$voting, log.pred > 0.3)
table(gerber$voting, type='response')
log.pred <- predict(log.model, table='response')
table(gerber$voting, log.pred > 0.3)
log.pred <- predict(log.model, type='response')
table(gerber$voting, log.pred > 0.3)
table(gerber$voting, log.pred > 0.5)
log.pred <- predict(log.model, type='response')
table(gerber$voting, log.pred > 0.5)
table(gerber$voting, log.pred < 0.5)
table(gerber$voting, log.pred < 0.3)
table(gerber$voting, log.pred > 0.3)
table(gerber$voting, log.pred > 0.5)
table(gerber$voting, log.pred >= 0.5)
str(log.pred>0.5)
levels(log.pred>0.5)
range(log.pred>0.5)
range(log.pred>0.3)
hist(log.pred)
log.pred <- predict(log.model, type='response')
hist(log.pred)
table(gerber$voting, log.pred >= 0.5)
235388/(235388+108696)
library(rpart)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
prp(CARTmodel)
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
prp(text)
plot(CARTmodel2)
plot(CARTmodel2$terms)
plot(text)
text(CARTmodel2)
CARTmodel3 = rpart(voting ~ sex + civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel3)
table(gerber$sex)
table(gerber$voting, log.pred >= 0.5)
plot(table(gerber$voting, log.pred >= 0.5))
setwd("../../../Code/R/DMC_task_2014/")
orders_class = read.csv("orders_class.txt")
orders_train = read.csv("orders_train.txt")
orders_class = read.delim("orders_class.txt", header=TRUE, sep=',')
head(orders_class)
orders_class = read.delim("orders_class.txt", header=TRUE, sep=';')
str(orders_class)
orders_train = read.delim("orders_train.txt", header=TRUE, sep=';')
str(orders_train)
range(orders_train$itemID)
levels(orders_train$itemID)
factor(orders_train$itemID)
table(orders_train$itemID)
hist(table(orders_train$itemID))
hist(table(orders_train$itemID, orders_train$returnShipment))
plot(table(orders_train$itemID, orders_train$returnShipment))
plot(orders_train$itemID, col = orders_train$returnShipment)
plot(orders_train$itemID)
plot(orders_train$itemID)
str(orders_train)
summary(orders_train)
emails = read.csv("energy_bids.csv", stringsAsFactors=FALSE)
setwd("../../../Courses/MITx/")
emails = read.csv("energy_bids.csv", stringsAsFactors=FALSE)
setwd("Analytics/")
emails = read.csv("energy_bids.csv", stringsAsFactors=FALSE)
str(emails)
emails$email[1]
emails$responsive[1]
emails$email[2]
emails$responsive[2]
# Responsive emails
table(emails$responsive)
# Video 3
# Load tm package
library(tm)
# Create corpus
corpus = Corpus(VectorSource(emails$email))
corpus[[1]]
# Pre-process data
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus[[1]]
install.packages("SnowballC")
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
# Look at first email
corpus[[1]]
corpus = Corpus(VectorSource(emails$email))
corpus[[1]]
# Pre-process data
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
# Look at first email
corpus[[1]]
corpus[[1]]
dtm = DocumentTermMatrix(corpus)
dtm
labeledTerms = as.data.frame(as.matrix(dtm))
labeledTerms$responsive = emails$responsive
str(labeledTerms)
library(caTools)
set.seed(144)
spl = sample.split(labeledTerms$responsive, 0.7)
train = subset(labeledTerms, spl == TRUE)
test = subset(labeledTerms, spl == FALSE)
library(rpart)
library(rpart.plot)
emailCART = rpart(responsive~., data=train, method="class")
prp(emailCART)
rm(list=ls())
wiki = read.csv("wiki.csv", stringsAsFactors=False )
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE  )
str(wiki)
table(wiki$Vandal)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded <- tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded <- tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
length(stopwords("english"))
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved <- tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved <- tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
wikiWords = cbind(wordsAdded, wordsRemoved, wiki$Vandal)
library(caTools)
set.seed(123)
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
split = sample.split(wikiWords$Vandal, splitRatio=0.7)
split = sample.split(wikiWords$Vandal, SplitRatio=0.7)
train = subset(wikiWords, split==TRUE)
test = subset(wikiWords, split==FALSE)
table(train$Vandal)
1443/2713
cartModel = rpart(Vandal ~., data=train, type='class')
cartModel = rpart(Vandal ~., data=train, method='class')
pred.cart = predict(cartModel, data=test, type='class')
table(test$Vandal, pred.cart>=0.5)
table(test$Vandal, pred.cart>0.5)
table(test$Vandal, pred.cart)
table(test$Vandal, as.numeric(pred.cart))
length(pred.cart)
pred.cart = predict(cartModel, newdata=test, type='class')
table(test$Vandal, as.numeric(pred.cart))
table(test$Vandal, (pred.cart))
table(test$Vandal, (pred.cart))
(618+12)/1163
library(rpart.plot)
prp(cartModel)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
str(wikiWords2)
table(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==TRUE)
cartModel2 = rpart(Vandal ~ . , data=wikiTrain2, method='class')
pred.cart2 = predict(cartModel2, newdata=wikiTest2, type = 'class')
prp(cartModel)
prp(cartModel2)
table(wikiTest2$Vandal, (pred.cart2))
wikiTest2 = subset(wikiWords2, split==FALSE)
cartModel2 = rpart(Vandal ~ . , data=wikiTrain2, method='class')
pred.cart2 = predict(cartModel2, newdata=wikiTest2, type = 'class')
prp(cartModel2)
pred.cart2 = predict(cartModel2, newdata=wikiTest2, type = 'class')
table(wikiTest2$Vandal, (pred.cart2))
(609+57)/1163
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
cartModel3 = rpart(Vandal ~ . , data=wikiTrain2, method='class')
pred.cart3 = predict(cartModel3, newdata=wikiTest2, type = 'class')
table(wikiTest2$Vandal, (pred.cart3))
(514+248)/1163
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain2 = subset(wikiWords3, split==TRUE)
wikiTest2 = subset(wikiWords3, split==FALSE)
cartModel3 = rpart(Vandal ~ . , data=wikiTrain2, method='class')
pred.cart3 = predict(cartModel3, newdata=wikiTest2, type = 'class')
table(wikiTest2$Vandal, (pred.cart3))
(595+241)/1163
prp(cartModel3)
rm(list=ls())
clinicalTrials = read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
str(clinicalTrials)
clinicalTrials = read.csv("clinical_trial.csv", stringsAsFactors=FALSE, fileEncoding='latin1')
str(clinicalTrials)
max(nchar(clinicalTrials$abstract))
count(nchar(clinicalTrials$abstract)==0)
sum(nchar(clinicalTrials$abstract)==0)
min(nchar(clinicalTrials$title))
library(tm)
titles = tm_map(clinicalTrials$title, toLower)
min(nchar(clinicalTrials$title))
which.min(nchar(clinicalTrials$title))
clinicalTrials$title[which.min(nchar(clinicalTrials$title))]
nchar("A decade of letrozole: FACE.")
nchar(A decade of letrozole: FACE.)
nchar('A decade of letrozole: FACE.')
clinicalTrials[which.min(nchar(clinicalTrials$title))]
clinicalTrials$title[which.min(nchar(clinicalTrials$title))]
trials$title[1258]
clinicalTrials$title[1258]
source('~/Dropbox/Courses/MITx/Analytics/week5_assignment.R', echo=TRUE)
dtmAbstract
max(dtmAbstract)
which.max(dtmAbstract)
str(dtmAbstract)
max(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = clinicalTrials$trial
str(dtm)
summary(dtm)
set.seed(144)
SplitRatio=0.7)
split = sample.split(dtm$trial, SplitRatio=0.7)
test = subset(dtm, split==FALSE)
train = subset(dtm, split==TRUE)
table(train$trial)
730/1302
trialCART = rpart(trial ~ . , data=train, method='class')
prp(trialCART)
pred.train = predict(trialCART)
max(pred.train[,2])
trainCA
trialCART
summary(trialCART)
table(train$trial,pred.train)
table(train$trial,pred.train[,1])
table(train$trial,pred.train[,1]>0.5)
table(train$trial,pred.train>0.5)
table(train$trial,pred.train[,2]>0.5)
(631+441)/1302
441/(441+131)
631/(631+99)
pred.test = predict(trialCART, newdata=test, type='class')
table(test$trial, pred.test)
(261+162)/558
library(ROCR)
pred = predict(trialCART, newdata=test)
pred.prob = pred[,2]
library(ROCR)
predROCR = prediction(pred.prob, test$responsive)
predROCR = prediction(pred.prob, test$trial)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
performance(predROCR, "auc")@y.values
rm(list=ls())
read.csv("emails.csv", stringAsFactors=FALSE)
read.csv("emails.csv", stringsAsFactors=FALSE)
emails = read.csv("emails.csv", stringsAsFactors=FALSE)
table(emails$spam)
emails$text[1]
max(nchar(emails$text))
which.min(nchar(emails$text))
corpusEmail = Corpus(VectorSource(emails$text))
corpusEmail <- tm_map(corpusEmail, tolower)
corpusEmail <- tm_map(corpusEmail, removePunctuation)
corpusEmail <- tm_map(corpusEmail, removeWords, stopwords("english"))
corpusEmail <- tm_map(corpusEmail, stemDocument)
dtmEmail = DocumentTermMatrix(corpusEmail)
dtmEmailSparse = removeSparseTerms(dtmEmail, 0.95)
spdtm = as.data.frame(as.matrix(dtmEmailSparse))
dtmEmail
dtmEmailSparse
emailsSparse = as.data.frame(as.matrix(spdtm))
head(emailsSparse)
names(emailsSparse)
which.max(colSums(emailsSparse))
emailsSparse$spam = emails$spam
sum(colSums(emailsSparse)>5000)
sum(colSums(emailsSparse[-1])>5000)
sum(colSums(emailsSparse[-1])>=5000)
sum(colSums(emailsSparse)>5000)
(colSums(emailsSparse)>5000)
emails[(colSums(emailsSparse)>5000)]
sum(colSums(emailsSparse)>5000)
sort(colSums(subset(emailsSparse, spam == 0)))
sum(colSums((subset(emails, emails$spam==0))) > 1000)
sum(colSums((subset(emailsSparse, spam==0))) > 1000)
sum(colSums((subset(emailsSparse, spam==1))) > 1000)
sum(colSums(emailsSparse) > 1000)
sort(colSums(subset(emailsSparse, spam == 1)))
emailsSparse$spam = as.factor(emailsSparse$spam)
set.seed(123)
split = sample.split(emailsSparse$spam , SplitRatio = 0.7)
train = subset(emailsSparse, split==TRUE)
test = subset(emailsSparse, split==FALSE)
spamLog = glm(spam ~ . , data = train, method='class')
spamLog = glm(spam ~ . , data = train, method='binomial')
spamLog = glm(spam ~ . , data = train, method='binomial', type='class')
spamCART = rpart(spam ~ . , data = train, method = "class")
set.seed(123)
library(randomForest)
set.seed(123)
spamRF = randomForest(spam ~ . , data = train, type = 'class')
spamRF = randomForest(spam ~ . , data = train)
library(MASS)
# take logs of the numerical data
lcrabs <- crabs
lcrabs[,4:8] <- log10(crabs[, 4:8])
# create vectors of labels for the observations
crabs.grp <- factor(c("B", "b", "O", "o")[rep(1:4, each = 50)])
crabs.grp2 <- factor(c("M","F","M","F")[rep(1:4,each = 50)])
# carry out PCA, and analyse variance
lcrabs.pca <- princomp(lcrabs[,4:8])
plot(lcrabs.pca)
summary(lcrabs.pca)
# look at 3D scatter plot of first 3 PCs
lcrabs.pc <- predict(lcrabs.pca)
library(rgl)
plot3d(lcrabs.pc[,1:3], type="s", size=0.4, col=crabs.grp)
# compare with scatter plot
library(lattice)
splom(lcrabs.pc[,1:3],col=crabs.grp)
splom(lcrabs.pc[,1:3],col=crabs.grp2)
.15*9.81/.1015
source('~/Dropbox/Code/ideatory/telr/svm_tuned.R', echo=TRUE)
setwd("../../../Code/ideatory//telr")
source('~/Dropbox/Code/ideatory/telr/svm_tuned.R', echo=TRUE)
ROCRpred = prediction(predictedClasses, datatest$Class)
as.numeric(performance(ROCRpred, "auc")@y.values)
source('~/Dropbox/Code/ideatory/telr/svm_tuned.R', echo=TRUE)
predictedClasses
datatest
datatest$Class
predictedClasses
dir()
setwd("../../")
dir()
setwd("../")
dir()
setwd("Courses/MITx/Analytics/")
im = read.csv("turbulence.gif")
im = read.table("turbulence.gif")
